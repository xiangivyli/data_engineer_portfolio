{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opendatasets in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (0.1.22)\n",
      "Requirement already satisfied: tqdm in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from opendatasets) (4.65.0)\n",
      "Requirement already satisfied: kaggle in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from opendatasets) (1.6.6)\n",
      "Requirement already satisfied: click in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from opendatasets) (8.0.4)\n",
      "Requirement already satisfied: six>=1.10 in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (1.16.0)\n",
      "Requirement already satisfied: certifi in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (2.8.2)\n",
      "Requirement already satisfied: requests in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (2.31.0)\n",
      "Requirement already satisfied: python-slugify in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (1.26.16)\n",
      "Requirement already satisfied: bleach in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (4.1.0)\n",
      "Requirement already satisfied: packaging in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from bleach->kaggle->opendatasets) (23.1)\n",
      "Requirement already satisfied: webencodings in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from requests->kaggle->opendatasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/xiangivyli/anaconda3/lib/python3.11/site-packages (from requests->kaggle->opendatasets) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import opendatasets as od\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://www.kaggle.com/datasets/arshkon/linkedin-job-postings?resource=download\"\n",
    "download_path = \"./airflow/include/dataset/2024-03-31/raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:Your Kaggle Key:Downloading linkedin-job-postings.zip to ./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.9M/44.9M [00:01<00:00, 30.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#enter username & key downloaded from https://kaggle.com/me/account create new api token\n",
    "od.download(dataset_url, data_dir=download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./airflow/include/dataset/2024-03-31/raw\u001b[00m\n",
      "└── \u001b[01;34mlinkedin-job-postings\u001b[00m\n",
      "    ├── \u001b[01;34mcompany_details\u001b[00m\n",
      "    │   ├── companies.csv\n",
      "    │   ├── company_industries.csv\n",
      "    │   ├── company_specialities.csv\n",
      "    │   └── employee_counts.csv\n",
      "    ├── \u001b[01;34mjob_details\u001b[00m\n",
      "    │   ├── benefits.csv\n",
      "    │   ├── job_industries.csv\n",
      "    │   ├── job_skills.csv\n",
      "    │   └── salaries.csv\n",
      "    ├── job_postings.csv\n",
      "    └── \u001b[01;34mmaps\u001b[00m\n",
      "        ├── industries.csv\n",
      "        └── skills.csv\n",
      "\n",
      "4 directories, 11 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./airflow/include/dataset/2024-03-31/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw/\n",
      "    linkedin-job-postings/\n",
      "        job_postings.csv\n",
      "        maps/\n",
      "            skills.csv\n",
      "            industries.csv\n",
      "        company_details/\n",
      "            employee_counts.csv\n",
      "            companies.csv\n",
      "            company_specialities.csv\n",
      "            company_industries.csv\n",
      "        job_details/\n",
      "            job_industries.csv\n",
      "            job_skills.csv\n",
      "            benefits.csv\n",
      "            salaries.csv\n"
     ]
    }
   ],
   "source": [
    "#list files\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f\"{subindent}{f}\")\n",
    "\n",
    "list_files('./airflow/include/dataset/2024-03-31/raw') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: ./airflow/include/dataset/2024-03-31/raw\n",
      "\n",
      "Directory: ./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings\n",
      "    job_postings.csv - 134845243 bytes\n",
      "\n",
      "Directory: ./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings/maps\n",
      "    skills.csv - 679 bytes\n",
      "    industries.csv - 6292 bytes\n",
      "\n",
      "Directory: ./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings/company_details\n",
      "    employee_counts.csv - 416749 bytes\n",
      "    companies.csv - 10953429 bytes\n",
      "    company_specialities.csv - 2036857 bytes\n",
      "    company_industries.csv - 390677 bytes\n",
      "\n",
      "Directory: ./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings/job_details\n",
      "    job_industries.csv - 666212 bytes\n",
      "    job_skills.csv - 925814 bytes\n",
      "    benefits.csv - 840280 bytes\n",
      "    salaries.csv - 731493 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get sizes of each file\n",
    "\n",
    "def get_size(start_path = '.'):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            if f.endswith('.csv'):\n",
    "                fp = os.path.join(dirpath, f)\n",
    "                total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "def print_file_sizes(start_path = '.'):\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        print(f\"Directory: {dirpath}\")\n",
    "        for f in filenames:\n",
    "            if f.endswith('.csv'):\n",
    "                fp = os.path.join(dirpath, f)\n",
    "                size = os.path.getsize(fp)\n",
    "                print(f\"    {f} - {size} bytes\")\n",
    "        print()  # Print a newline for readability\n",
    "\n",
    "# Set the start path to data directory\n",
    "data_directory = './airflow/include/dataset/2024-03-31/raw'  # Adjust this to the path where data resides\n",
    "print_file_sizes(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings/job_postings.csv: 33246 rows\n",
      "./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings/maps/skills.csv: 35 rows\n",
      "./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings/maps/industries.csv: 229 rows\n",
      "./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings/company_details/employee_counts.csv: 14275 rows\n",
      "./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings/company_details/companies.csv: 11361 rows\n",
      "./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings/company_details/company_specialities.csv: 78405 rows\n",
      "./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings/company_details/company_industries.csv: 12601 rows\n",
      "./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings/job_details/job_industries.csv: 44091 rows\n",
      "./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings/job_details/job_skills.csv: 56591 rows\n",
      "./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings/job_details/benefits.csv: 29325 rows\n",
      "./airflow/include/dataset/2024-03-31/raw/linkedin-job-postings/job_details/salaries.csv: 13352 rows\n"
     ]
    }
   ],
   "source": [
    "# get the number of records\n",
    "\n",
    "def print_csv_row_counts(start_path='.'):\n",
    "    for dirpath, _, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            if f.endswith('.csv'):\n",
    "                file_path = os.path.join(dirpath, f)\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    print(f\"{file_path}: {len(df.index)} rows\")\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    print(f\"{file_path}: Empty\")\n",
    "                except Exception as e:\n",
    "                    print(f\"{file_path}: Failed to read ({e})\")\n",
    "\n",
    "# Set the start path \n",
    "data_directory = './airflow/include/dataset/2024-03-31/raw'  \n",
    "print_csv_row_counts(data_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
